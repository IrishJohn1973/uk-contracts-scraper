name: UK Contracts (daily)

on:
  schedule:
    # 07:15 UTC every day (08:15 Dublin during summer time)
    - cron: "15 7 * * *"
  workflow_dispatch:
    inputs:
      pages:
        description: "How many listing pages to scan (via ?page=)"
        required: false
        default: "50"
      max_details:
        description: "Cap on detail pages to fetch/parse"
        required: false
        default: "5000"

concurrency:
  group: uk-cf-daily
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      PAGES: ${{ inputs.pages || '50' }}
      MAX_DETAILS: ${{ inputs.max_details || '5000' }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install psql
        run: sudo apt-get update && sudo apt-get install -y postgresql-client

      - name: Sanity check DB
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "${DATABASE_URL:-}" ]; then
            echo "ERROR: Missing DATABASE_URL secret on repo."
            exit 1
          fi
          node -e "const u=new URL(process.env.DATABASE_URL); console.log('DB host:', u.hostname)"
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -c "select version();"

      - name: Install npm deps
        run: npm ci

      # Crawl & backfill recent listings (yesterday coverage comes from early pages; idempotent)
      - name: Archive results pages
        run: node scripts/uk_cf_archive_results.mjs "$PAGES" | tee uk_cf_archive_results.log

      - name: Backfill from listing pages (?page=)
        run: node scripts/uk_cf_backfill_results_pages.mjs "$PAGES" | tee uk_cf_backfill_results_pages.log

      # Fetch detail HTML into RAW, then extract descriptions and parse
      - name: Fetch detail HTML (gz)
        run: node scripts/uk_cf_fetch_details_gz.mjs "$MAX_DETAILS" | tee uk_cf_fetch_details_gz.log

      - name: Extract descriptions
        run: node scripts/uk_cf_extract_descriptions.mjs "$MAX_DETAILS" | tee uk_cf_extract_descriptions.log

      - name: Link staging.raw_id from latest RAW detail
        shell: bash
        run: |
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -c "
          WITH latest_detail AS (
            SELECT DISTINCT ON (source_id) source_id, raw_id, inserted_at
            FROM public.uk_raw_std
            WHERE source='uk_cf' AND kind='detail' AND url LIKE '%/notice/%'
            ORDER BY source_id, inserted_at DESC
          )
          UPDATE public.uk_staging_std s
          SET raw_id = d.raw_id
          FROM latest_detail d
          WHERE s.source='uk_cf' AND s.raw_id IS NULL AND s.source_id = d.source_id;
          "

      - name: Parse details (enhanced)
        run: node scripts/uk_cf_parse_details_enhanced.mjs "$MAX_DETAILS" | tee uk_cf_parse_details_enhanced.log

      - name: Export preview CSV (last 300 by published_at/deadline)
        shell: bash
        run: |
          psql "$DATABASE_URL" -Atc "COPY (
            SELECT uk_uid,
                   title,
                   buyer_name,
                   notice_type,
                   nuts,
                   published_at,
                   deadline,
                   COALESCE(array_length(cpv_codes,1),0) AS cpv_count,
                   LEFT(COALESCE(short_desc,''), 240) AS short_desc_preview,
                   source_url
            FROM public.uk_staging_std
            WHERE source='uk_cf'
            ORDER BY COALESCE(published_at, deadline, now()) DESC
            LIMIT 300
          ) TO STDOUT WITH CSV HEADER" > uk_cf_latest.csv

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: uk-cf-daily-artifacts
          path: |
            *.log
            uk_cf_latest.csv
          if-no-files-found: warn
