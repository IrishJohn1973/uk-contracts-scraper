name: UK Contracts (daily)

on:
  workflow_dispatch:
    inputs:
      PAGES:
        description: "How many results pages to backfill via ?page="
        required: false
        default: "10"
      MAX_DETAILS:
        description: "Max detail rows to process"
        required: false
        default: "2000"
  schedule:
    - cron: "15 5 * * *" # 05:15 UTC daily

jobs:
  run:
    runs-on: ubuntu-24.04
    timeout-minutes: 180
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      PAGES: ${{ github.event.inputs.PAGES || '60' }}
      MAX_DETAILS: ${{ github.event.inputs.MAX_DETAILS || '5000' }}
      # CI-only: relax TLS so node-postgres doesn't fail on "self-signed certificate in certificate chain"
      NODE_TLS_REJECT_UNAUTHORIZED: "0"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install psql
        run: sudo apt-get update && sudo apt-get install -y postgresql-client

      - name: Sanity check DB
        shell: bash
        run: |
          set -euo pipefail
          node -e "const u=new URL(process.env.DATABASE_URL); console.log('DB host:', u.hostname)"
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -c "select version();"

      - name: Install npm deps
        run: npm ci

      - name: Archive results pages
        run: node scripts/uk_cf_archive_results.mjs "$PAGES" | tee uk_cf_archive_results.log

      - name: Backfill from listing pages (?page=)
        run: node scripts/uk_cf_backfill_results_pages.mjs "$PAGES" | tee uk_cf_backfill_results_pages.log

      - name: Fetch detail HTML (gz)
        run: node scripts/uk_cf_fetch_details_gz.mjs "$MAX_DETAILS" | tee uk_cf_fetch_details_gz.log

      - name: Extract descriptions
        run: node scripts/uk_cf_extract_descriptions.mjs "$MAX_DETAILS" | tee uk_cf_extract_descriptions.log

      - name: Link staging.raw_id from latest RAW detail
        shell: bash
        run: |
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -c "
          WITH latest_detail AS (
            SELECT DISTINCT ON (source_id) source_id, raw_id, inserted_at
            FROM public.uk_raw_std
            WHERE source='uk_cf' AND kind='detail' AND url LIKE '%/notice/%'
            ORDER BY source_id, inserted_at DESC
          )
          UPDATE public.uk_staging_std s
          SET raw_id = d.raw_id
          FROM latest_detail d
          WHERE s.source='uk_cf' AND s.raw_id IS NULL AND s.source_id = d.source_id;
          "

      - name: Parse details (enhanced)
        run: node scripts/uk_cf_parse_details_enhanced.mjs "$MAX_DETAILS" | tee uk_cf_parse_details_enhanced.log

      - name: Export preview CSV (last 300 by published_at/deadline)
        shell: bash
        run: |
          psql "$DATABASE_URL" -Atc "COPY (
            SELECT uk_uid,
                   title,
                   buyer_name,
                   notice_type,
                   nuts,
                   published_at,
                   deadline,
                   COALESCE(array_length(cpv_codes,1),0) AS cpv_count,
                   LEFT(COALESCE(short_desc,''), 240) AS short_desc_preview,
                   source_url
            FROM public.uk_staging_std
            WHERE source='uk_cf'
            ORDER BY COALESCE(published_at, deadline, now()) DESC
            LIMIT 300
          ) TO STDOUT WITH CSV HEADER" > uk_cf_latest.csv

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: uk-cf-daily-artifacts
          path: |
            *.log
            uk_cf_latest.csv
